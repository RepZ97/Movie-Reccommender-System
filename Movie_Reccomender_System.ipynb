{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Reccomender System"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from numpy import loadtxt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Ratings and Rated Movies Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./data2/small_movies_Y.csv', 'rb')\n",
    "Y = loadtxt(file,delimiter = \",\")\n",
    "file = open('./data2/small_movies_R.csv', 'rb')\n",
    "R = loadtxt(file,delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting number of movies and users\n",
    "num_movies = Y.shape[0]\n",
    "num_users = Y.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Movie List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML = pd.read_csv('./data2/small_movie_list.csv', header=0, index_col=0, delimiter=',', quotechar='\"')\n",
    "mlist = dfML[\"title\"].to_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting New User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " New User Ratings:\n",
      "\n",
      "Rated 3.0 for Final Destination (2000)\n",
      "Rated 2.0 for Flintstones in Viva Rock Vegas, The (2000)\n",
      "Rated 4.0 for Chicken Run (2000)\n",
      "Rated 2.0 for X-Men (2000)\n",
      "Rated 4.0 for 40-Year-Old Virgin, The (2005)\n",
      "Rated 1.0 for Ghost Rider (2007)\n",
      "Rated 3.0 for Hot Tub Time Machine (2010)\n",
      "Rated 2.0 for Kick-Ass (2010)\n",
      "Rated 4.0 for MacGruber (2010)\n",
      "Rated 5.0 for Inception (2010)\n",
      "Rated 3.0 for Grown Ups (2010)\n",
      "Rated 4.0 for 22 Jump Street (2014)\n",
      "Rated 3.0 for Sex Tape (2014)\n",
      "Rated 4.0 for Guardians of the Galaxy (2014)\n",
      "Rated 2.0 for The Imitation Game (2014)\n"
     ]
    }
   ],
   "source": [
    "# Initializing new user ratings\n",
    "new_user_ratings = np.zeros(num_movies)\n",
    "\n",
    "new_user_ratings[24] = 3\n",
    "new_user_ratings[46] = 2\n",
    "new_user_ratings[70] = 4\n",
    "new_user_ratings[79] = 2\n",
    "new_user_ratings[3689] = 4\n",
    "\n",
    "new_user_ratings[3711] = 3\n",
    "new_user_ratings[3714] = 4\n",
    "new_user_ratings[3802] = 2\n",
    "new_user_ratings[2647] = 3\n",
    "new_user_ratings[2653] = 2\n",
    "\n",
    "new_user_ratings[2672] = 4\n",
    "new_user_ratings[2716] = 5\n",
    "new_user_ratings[2717] = 3\n",
    "new_user_ratings[1444] = 4\n",
    "new_user_ratings[1832] = 1\n",
    "\n",
    "new_user_rated = [i for i in range(len(new_user_ratings)) if new_user_ratings[i] > 0]\n",
    "\n",
    "print('\\n New User Ratings:\\n')\n",
    "for i in range(len(new_user_ratings)):\n",
    "    if new_user_ratings[i] > 0 :\n",
    "        print(f'Rated {new_user_ratings[i]} for {dfML.loc[i,\"title\"]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add New User Ratings to the Already Rated Data and Normalize Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.c_[new_user_ratings, Y]\n",
    "\n",
    "# Add new user ratings indicators to R\n",
    "R = np.c_[(new_user_ratings != 0).astype(int), R]\n",
    "\n",
    "# Normalize the Dataset\n",
    "Ymean = (np.sum(Y*R,axis=1)/(np.sum(R, axis=1)+1e-12)).reshape(-1,1)\n",
    "Ynorm = Y - np.multiply(Ymean, R)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Cost Function with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(X, W, b, Y, R, lambda_):\n",
    "    nm, nu = Y.shape\n",
    "    J = 0\n",
    "    Regt1 = 0\n",
    "    Regt2 = 0\n",
    " \n",
    "    for i in range(nm):\n",
    "        for j in range(nu):\n",
    "            J += (R[i][j]*np.square((np.dot(W[j],X[i])+b[0][j]-Y[i][j])))/2\n",
    "            \n",
    "        Regt2 += (np.sum(np.square(X[i])))*(lambda_/2)\n",
    "        \n",
    "    for j in range(nu):\n",
    "        Regt1 += (np.sum(np.square(W[j])))*(lambda_/2)\n",
    "    \n",
    "    J += Regt1 + Regt2\n",
    "\n",
    "    return J"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized Cost Function Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func_v(X, W, b, Y, R, lambda_):\n",
    "    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing to Train the Model by Initializing Prameters and Selecting the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500 features are used\n",
    "num_movies, num_users = Y.shape\n",
    "num_features = 200\n",
    "\n",
    "# Setting initial learnable parameters (W,X)\n",
    "tf.random.set_seed(1234)\n",
    "W = tf.Variable(tf.random.normal((num_users,  num_features),dtype=tf.float64),  name='W')\n",
    "X = tf.Variable(tf.random.normal((num_movies, num_features),dtype=tf.float64),  name='X')\n",
    "b = tf.Variable(tf.random.normal((1, num_users),   dtype=tf.float64),  name='b')\n",
    "\n",
    "# Adam Optimizer is used with learning rate of 0.1\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of Y (4778, 445)\n",
      "shape of X (4778, 200)\n",
      "shape of W (445, 200)\n",
      "shape of R (4778, 445)\n"
     ]
    }
   ],
   "source": [
    "print(f'shape of Y {Y.shape}')\n",
    "print(f'shape of X {X.shape}')\n",
    "print(f'shape of W {W.shape}')\n",
    "print(f'shape of R {R.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at iteration 0: 4540906.4\n",
      "Training loss at iteration 20: 318068.1\n",
      "Training loss at iteration 40: 137926.3\n",
      "Training loss at iteration 60: 71411.6\n",
      "Training loss at iteration 80: 41396.0\n",
      "Training loss at iteration 100: 25923.7\n",
      "Training loss at iteration 120: 17171.5\n",
      "Training loss at iteration 140: 11906.8\n",
      "Training loss at iteration 160: 8602.2\n",
      "Training loss at iteration 180: 6462.7\n",
      "Training loss at iteration 200: 5045.2\n",
      "Training loss at iteration 220: 4089.5\n",
      "Training loss at iteration 240: 3436.1\n",
      "Training loss at iteration 260: 2984.0\n",
      "Training loss at iteration 280: 2667.6\n",
      "Training loss at iteration 300: 2443.6\n",
      "Training loss at iteration 320: 2282.9\n",
      "Training loss at iteration 340: 2166.3\n",
      "Training loss at iteration 360: 2080.5\n",
      "Training loss at iteration 380: 2016.4\n",
      "Training loss at iteration 400: 1968.0\n",
      "Training loss at iteration 420: 1930.8\n",
      "Training loss at iteration 440: 1902.0\n",
      "Training loss at iteration 460: 1879.3\n",
      "Training loss at iteration 480: 1861.3\n",
      "Training loss at iteration 500: 1846.7\n",
      "Training loss at iteration 520: 1834.9\n",
      "Training loss at iteration 540: 1825.1\n",
      "Training loss at iteration 560: 1817.0\n",
      "Training loss at iteration 580: 1810.3\n",
      "Training loss at iteration 600: 1804.6\n",
      "Training loss at iteration 620: 1799.7\n",
      "Training loss at iteration 640: 1795.5\n",
      "Training loss at iteration 660: 1792.0\n",
      "Training loss at iteration 680: 1788.9\n",
      "Training loss at iteration 700: 1786.2\n",
      "Training loss at iteration 720: 1783.9\n",
      "Training loss at iteration 740: 1781.8\n",
      "Training loss at iteration 760: 1780.0\n",
      "Training loss at iteration 780: 1778.5\n",
      "Training loss at iteration 800: 1777.1\n",
      "Training loss at iteration 820: 1775.8\n",
      "Training loss at iteration 840: 1774.7\n",
      "Training loss at iteration 860: 1773.7\n",
      "Training loss at iteration 880: 1772.9\n",
      "Training loss at iteration 900: 1772.1\n",
      "Training loss at iteration 920: 1771.4\n",
      "Training loss at iteration 940: 1770.8\n",
      "Training loss at iteration 960: 1770.2\n",
      "Training loss at iteration 980: 1769.7\n"
     ]
    }
   ],
   "source": [
    "# Initializing number of iterations and the regualarization parameter\n",
    "iterations = 1000\n",
    "lambda_ = 1\n",
    "\n",
    "# Custom Training Loop\n",
    "for iter in range(iterations):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # computing the cost\n",
    "        cost_value = cost_func_v(X, W, b, Ynorm, R, lambda_)\n",
    "\n",
    "    # Using gradient tape to retrieve the gradients\n",
    "    grads = tape.gradient( cost_value, [X,W,b])\n",
    "\n",
    "    # update the values of varibles to minimize the loss\n",
    "    optimizer.apply_gradients( zip(grads, [X,W,b]))\n",
    "\n",
    "    #logging the process\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"Training loss at iteration {iter}: {cost_value:0.1f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Reccommendations to New User Based on Movies with Highest Predicted Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of Y (4778, 444)\n",
      "shape of X (4778, 200)\n",
      "shape of W (444, 200)\n",
      "shape of R (4778, 444)\n"
     ]
    }
   ],
   "source": [
    "print(f'shape of Y {Y.shape}')\n",
    "print(f'shape of X {X.shape}')\n",
    "print(f'shape of W {W.shape}')\n",
    "print(f'shape of R {R.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting rating 4.80 for movie Odd Life of Timothy Green, The (2012)\n",
      "Predicting rating 4.75 for movie Into the Forest of Fireflies' Light (2011)\n",
      "Predicting rating 4.73 for movie Dragons: Gift of the Night Fury (2011)\n",
      "Predicting rating 4.73 for movie 'Salem's Lot (2004)\n",
      "Predicting rating 4.73 for movie Satin Rouge (2002)\n",
      "Predicting rating 4.72 for movie The Girl with All the Gifts (2016)\n",
      "Predicting rating 4.72 for movie 9/11 (2002)\n",
      "Predicting rating 4.72 for movie Black Tar Heroin: The Dark End of the Street (2000)\n",
      "Predicting rating 4.72 for movie Idiots and Angels (2008)\n",
      "Predicting rating 4.72 for movie Wonder Woman (2009)\n",
      "Predicting rating 4.72 for movie Faster (2010)\n",
      "Predicting rating 4.72 for movie Justice League: Doom (2012) \n",
      "Predicting rating 4.72 for movie A Detective Story (2003)\n",
      "Predicting rating 4.72 for movie Superman/Batman: Public Enemies (2009)\n",
      "Predicting rating 4.72 for movie Open Hearts (Elsker dig for evigt) (2002)\n",
      "Predicting rating 4.72 for movie Palindromes (2004)\n",
      "Predicting rating 4.72 for movie Strictly Sexual (2008)\n",
      "Predicting rating 4.72 for movie Happy Feet Two (2011)\n",
      "Predicting rating 4.72 for movie Miss Nobody (2010)\n",
      "\n",
      "\n",
      "Original vs Predicted ratings:\n",
      "\n",
      "Original 3.0, Predicted 2.95 for Final Destination (2000)\n",
      "Original 2.0, Predicted 1.80 for Flintstones in Viva Rock Vegas, The (2000)\n",
      "Original 4.0, Predicted 3.91 for Chicken Run (2000)\n",
      "Original 2.0, Predicted 2.19 for X-Men (2000)\n",
      "Original 4.0, Predicted 3.93 for 40-Year-Old Virgin, The (2005)\n",
      "Original 1.0, Predicted 1.20 for Ghost Rider (2007)\n",
      "Original 3.0, Predicted 2.94 for Hot Tub Time Machine (2010)\n",
      "Original 2.0, Predicted 2.25 for Kick-Ass (2010)\n",
      "Original 4.0, Predicted 3.94 for MacGruber (2010)\n",
      "Original 5.0, Predicted 4.92 for Inception (2010)\n",
      "Original 3.0, Predicted 3.05 for Grown Ups (2010)\n",
      "Original 4.0, Predicted 3.81 for 22 Jump Street (2014)\n",
      "Original 3.0, Predicted 2.80 for Sex Tape (2014)\n",
      "Original 4.0, Predicted 3.87 for Guardians of the Galaxy (2014)\n",
      "Original 2.0, Predicted 2.41 for The Imitation Game (2014)\n"
     ]
    }
   ],
   "source": [
    "# Making predictions with trained weights and biases\n",
    "p = np.matmul(X.numpy(), np.transpose(W.numpy())) + b.numpy()\n",
    "\n",
    "# Restoring the mean that we substracted during regularization process\n",
    "pm = p + Ymean\n",
    "\n",
    "new_predictions = pm[:,0]\n",
    "\n",
    "# sort predictions\n",
    "sp = tf.argsort(new_predictions, direction='DESCENDING')\n",
    "\n",
    "for i in range(20):\n",
    "    j = sp[i]\n",
    "    if new_predictions[j] > 5:\n",
    "        new_predictions[j] = 5\n",
    "    if j not in new_user_rated:\n",
    "        print(f'Predicting rating {new_predictions[j]:0.2f} for movie {mlist[j]}')\n",
    "\n",
    "print('\\n\\nOriginal vs Predicted ratings:\\n')\n",
    "for i in range(len(new_user_ratings)):\n",
    "    if new_predictions[i] > 5:\n",
    "        new_predictions[i] = 5\n",
    "    if new_user_ratings[i] > 0:\n",
    "        print(f'Original {new_user_ratings[i]}, Predicted {new_predictions[i]:0.2f} for {mlist[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a328c16d601b4d372c2f1f8080dc93b0cd84e484b40e0f1bc5be9b1a4a0d9797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
